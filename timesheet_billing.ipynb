{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database_utils import *\n",
    "clickhouse_client = gong_cha_redcat_db_clickhouse_client\n",
    "\n",
    "# # # START OF FUNCTIONS\n",
    "def extract_additional_hr(file, sheet_name):\n",
    "  df = pd.read_excel(file, sheet_name = sheet_name)\n",
    "  df = df.dropna(subset=['Employee ID'])\n",
    "\n",
    "  if df['Employee ID'].dtypes == 'object':\n",
    "    df['Employee ID'] = df['Employee ID'].str[:6]\n",
    "  df['Employee ID'] = df['Employee ID'].astype(int)\n",
    "\n",
    "  if df['Store'].dtypes == float:\n",
    "    df['Store'] = df['Store'].astype(int).astype(str)\n",
    "\n",
    "  df = df.fillna({'Add': 0, 'Add.1': 0,'Add.2': 0,'Add.3': 0,'Add.4': 0,'Add.5': 0,'Personal Leave': 0,'Annual Leave': 0,})\n",
    "  df['2012-11-01'] = df['Add'] + df['Add.1'] + df['Add.2'] + df['Add.3'] + df['Add.4'] + df['Add.5']\n",
    "  df['2013-01-01'] = df['Personal Leave'] + df['Annual Leave']\n",
    "  cols =[0,1,2, df.columns.tolist().index('2012-11-01'),df.columns.tolist().index('2013-01-01')]\n",
    "\n",
    "  df = df.iloc[:, cols]\n",
    "  df = df.dropna(subset=df.columns[:3], how='all')\n",
    "  df = df.melt(id_vars=['Employee ID', 'Store', 'Preferred Name'], value_vars=df.columns[3:], var_name='Date', value_name='Hours')\n",
    "  df = df[df['Hours'] != 0]\n",
    "  return df\n",
    "\n",
    "def extract_rostered_hr(file, sheet_name):\n",
    "  df = pd.read_excel(file, sheet_name = sheet_name)\n",
    "\n",
    "  df = df.dropna(subset=['Employee ID'])\n",
    "\n",
    "  if df['Employee ID'].dtypes == 'object':\n",
    "    df['Employee ID'] = df['Employee ID'].str[:6]\n",
    "  df['Employee ID'] = df['Employee ID'].astype(int)\n",
    "\n",
    "  if df['Store'].dtypes == float:\n",
    "    df['Store'] = df['Store'].astype(int).astype(str)\n",
    "\n",
    "  df = df.iloc[:, :24]\n",
    "  cols = [0,1,2]\n",
    "  for col in range(3, 24, 3):\n",
    "      df[df.columns[col]] = df[df.columns[col+2]]\n",
    "      cols.append(col)\n",
    "  df = df.iloc[:,cols]\n",
    "  df = df.dropna(subset=df.columns[:3], how='all')\n",
    "  df = df.melt(id_vars=['Employee ID', 'Store', 'Preferred Name'], value_vars=df.columns[3:], var_name='Date', value_name='Hours')\n",
    "  df = df[df['Hours'] != 0]\n",
    "  return df\n",
    "\n",
    "# Function to count digits before the decimal point in a float number\n",
    "def count_digits_before_decimal(number):\n",
    "  # Convert to string, split at the decimal point, and count digits in the integer part\n",
    "  integer_part = str(number).split('.')[0]\n",
    "  return len(integer_part.replace('-', '').replace('nan', ''))\n",
    "\n",
    "def calc_timesheets_n_billings(files):\n",
    "  print('calc')\n",
    "  timesheets = pd.DataFrame()\n",
    "  billings = pd.DataFrame()\n",
    "  rostered_hr = pd.DataFrame()\n",
    "  additional_hr = pd.DataFrame()\n",
    "\n",
    "  for file in files:\n",
    "    timesheet = pd.read_excel(file, sheet_name = 'Timesheet')\n",
    "    billing = pd.read_excel(file, sheet_name = 'Billing')\n",
    "    rostered_hr_w1 = extract_rostered_hr(file, 'Week 1 Roster')\n",
    "    rostered_hr_w2 = extract_rostered_hr(file, 'Week 2 Roster')\n",
    "    additional_hr_w1 = extract_additional_hr(file, 'Week 1 Roster')\n",
    "    additional_hr_w2 = extract_additional_hr(file, 'Week 2 Roster')\n",
    "    employees = pd.read_excel(file, sheet_name = 'Employees')\n",
    "\n",
    "    timesheets = pd.concat([timesheets, timesheet], ignore_index=True)\n",
    "    billings = pd.concat([billings, billing], ignore_index=True)\n",
    "    rostered_hr = pd.concat([rostered_hr, rostered_hr_w1], ignore_index=True)\n",
    "    rostered_hr = pd.concat([rostered_hr, rostered_hr_w2], ignore_index=True)\n",
    "    additional_hr = pd.concat([additional_hr, additional_hr_w1], ignore_index=True)\n",
    "    additional_hr = pd.concat([additional_hr, additional_hr_w2], ignore_index=True)\n",
    "\n",
    "  roster_start_date = rostered_hr['Date'].min()\n",
    "  #Remove irrelevant rows\n",
    "  timesheets = timesheets.dropna(subset = ['Employee ID'])\n",
    "\n",
    "  #Keep the needed columns\n",
    "  timesheets_cols = [1,2,3,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "  timesheets = timesheets[timesheets.columns[timesheets_cols]]\n",
    "  timesheets['Update Wage'] = timesheets['Update Wage'].astype(bool)\n",
    "  #Column Aggregations\n",
    "  timesheets_agg_cols = {'First Name':'first','Last Name':'first','Update Wage':'first','Hour Threshold':'first','Company':'first','Ord':'sum','Sat':'sum','Sun':'sum','Pub':'sum','Eve 1':'sum','Eve 2':'sum','No. of Shifts':'sum','Personal Leave':'sum','Annual Leave':'sum','Unpaid Leave':'sum','Total':'sum'}\n",
    "  timesheets = timesheets.groupby('Employee ID', as_index = False).agg(timesheets_agg_cols)\n",
    "  #Calculate Over Threshold\n",
    "  timesheets['Over Threshold'] = timesheets['Total'] - timesheets['Hour Threshold']\n",
    "  timesheets.loc[timesheets[\"Over Threshold\"] <=0, \"Over Threshold\"] = 0\n",
    "  #Keep Over Thresholds to a new df\n",
    "  over_threshold = timesheets[timesheets['Over Threshold']>0]\n",
    "  #Reduce Ord & Total with the excess\n",
    "  timesheets['Ord'] = timesheets['Ord'] - timesheets['Over Threshold']\n",
    "  timesheets['Total'] = timesheets['Total'] - timesheets['Over Threshold']\n",
    "  #Convert 80 & 100 hours to 76 hours\n",
    "  hours_col = ['Ord', 'Sat','Sun','Eve 1','Eve 2','Pub','Personal Leave', 'Annual Leave', 'Unpaid Leave', 'Total']\n",
    "  if(100 in timesheets[\"Hour Threshold\"].values):\n",
    "      timesheets.loc[timesheets[\"Hour Threshold\"] == 100, hours_col] = timesheets[hours_col]/100*76\n",
    "  if(80 in timesheets[\"Hour Threshold\"].values):\n",
    "      timesheets.loc[timesheets[\"Hour Threshold\"] == 80, hours_col] = timesheets[hours_col]/80*76\n",
    "  if any(timesheets[\"Hour Threshold\"] > 1000):\n",
    "  # Find rows where \"Hour Threshold\" is greater than 1000\n",
    "      rows_to_update = timesheets.loc[timesheets[\"Hour Threshold\"] > 1000]\n",
    "      # Perform actions on the rows\n",
    "      for index, row in rows_to_update.iterrows():\n",
    "          threshold = int(row[\"Hour Threshold\"])\n",
    "          base = int(str(threshold)[:2])\n",
    "          conversion = int(str(threshold)[-2:])\n",
    "          # Update multiple columns using .loc\n",
    "          timesheets.loc[index, hours_col] = timesheets.loc[index, hours_col] / conversion * base\n",
    "\n",
    "  #drop Hour Threshold & Over Threshold\n",
    "  timesheets = timesheets.drop(['Hour Threshold','Over Threshold'],axis = 1)\n",
    "\n",
    "  #Remove irrelevant rows\n",
    "  billings.dropna(subset=['Store'],inplace = True)\n",
    "  billings = billings[billings['Total'] > 0]\n",
    "\n",
    "  #Keep the needed columns\n",
    "  billings_cols = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "  billings = billings[billings.columns[billings_cols]]\n",
    "  #Column Aggregations\n",
    "  billings_agg_cols = {'Ord':'sum','Sat':'sum','Sun':'sum','Pub':'sum','Eve 1':'sum','Eve 2':'sum','No. of Shifts':'sum','Personal Leave':'sum','Annual Leave':'sum','Unpaid Leave':'sum','Total':'sum'}\n",
    "  billings = billings.groupby('Store', as_index = False).agg(billings_agg_cols)\n",
    "\n",
    "  rostered_hr = pd.merge(rostered_hr, employees[['Employee ID', 'First Name', 'Last Name', 'Company']], how='left', on=['Employee ID'])\n",
    "  rostered_hr_col = [\n",
    "  'Employee ID',\n",
    "  'First Name',\n",
    "  'Last Name',\n",
    "  'Preferred Name',\n",
    "  'Company',\n",
    "  'Store',\n",
    "  'Date',\n",
    "  'Hours'\n",
    "  ]\n",
    "  rostered_hr = rostered_hr[rostered_hr_col]\n",
    "  rostered_hr['Date'] = pd.to_datetime(rostered_hr['Date']).dt.date\n",
    "\n",
    "  bonus = rostered_hr.copy()\n",
    "\n",
    "  # Stitch Store ID and drop rows which Store ID are not found\n",
    "  sheet_id = '1rqOeBjA9drmTnjlENvr57RqL5-oxSqe_KGdbdL2MKhM'\n",
    "  sheet_name = 'StoreReference'\n",
    "  url = f'https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}'\n",
    "  store_ref = pd.read_csv(url)\n",
    "\n",
    "  bonus = pd.merge(bonus, store_ref, on=['Store'], how = 'left')\n",
    "\n",
    "  bonus.dropna(subset=['Store ID'], inplace = True)\n",
    "\n",
    "  if bonus.shape[0] == 0:\n",
    "     print('No bonus to process')\n",
    "  \n",
    "  else:\n",
    "    store_crm_config = {\n",
    "      'sheet_id': '1aVcnah9Cp_PUvFiXgd2XRmBpLhumCqHUaqROaLcpYfc',\n",
    "      'sheet_name': 'store'\n",
    "    }\n",
    "    store_crm = read_csv_from_config(store_crm_config)\n",
    "    store_crm['opened_on'] = pd.to_datetime(store_crm['opened_on'])\n",
    "    store_crm['closed_on'] = pd.to_datetime(store_crm['closed_on'])\n",
    "    store_crm_col = ['store_id', 'recid_plo', 'menu_type']\n",
    "    store_crm = store_crm[store_crm_col]\n",
    "    store_crm = store_crm[~store_crm['recid_plo'].isna()]\n",
    "    store_crm = store_crm.rename(columns={'store_id': 'Store ID'})\n",
    "    \n",
    "    bonus = pd.merge(bonus, store_crm[['Store ID', 'recid_plo']], on=['Store ID'], how = 'left')\n",
    "    bonus['recid_plo'] = bonus['recid_plo'].astype(int)\n",
    "\n",
    "    # Stich sales based on recid_plo & dates, skip if there is no Date\n",
    "    start = bonus['Date'].min()\n",
    "    end = bonus['Date'].max()\n",
    "\n",
    "    start_str = start.strftime('%Y-%m-%d')\n",
    "    end_str = end.strftime('%Y-%m-%d')\n",
    "\n",
    "    recid_plo_list = bonus['recid_plo'].unique().tolist()\n",
    "    recid_plo_list_str = ', '.join(str(id) for id in recid_plo_list)\n",
    "\n",
    "    query = '''\n",
    "    SELECT *\n",
    "    FROM r_opsbonusexclusion\n",
    "    '''\n",
    "    exclusion_df = get_DataFrame_from_clickhouse(query, clickhouse_client)\n",
    "    excluded_recid_plu = exclusion_df['recid_plu'].drop_duplicates()\n",
    "    excluded_recid_plu_str = ', '.join(str(s) for s in excluded_recid_plu)\n",
    "\n",
    "    query = '''\n",
    "    SELECT recid_plo, itemdate as Date, sum(net_amount + gst_amount) as Sales\n",
    "    FROM d_txnlines\n",
    "    WHERE itemdate >='{start}' and itemdate <= '{end}' and recid_plo in ({recid_plo_list}) and recid_plu not in ({excluded_recid_plu})\n",
    "    GROUP BY recid_plo, itemdate\n",
    "    ORDER BY itemdate ASC, recid_plo ASC\n",
    "    '''.format(start=start_str, end = end_str, recid_plo_list = recid_plo_list_str, excluded_recid_plu = excluded_recid_plu_str)\n",
    "    sales_df = get_DataFrame_from_clickhouse(query, clickhouse_client)\n",
    "\n",
    "    sales_df['Date'] = pd.to_datetime(sales_df['Date']).dt.date\n",
    "\n",
    "    bonus = pd.merge(bonus, sales_df[['recid_plo', 'Date', 'Sales']], on=['recid_plo', 'Date'], how = 'left')\n",
    "\n",
    "    # Stitch Target Sales & Bonus Rates\n",
    "    sheet_id = '1rqOeBjA9drmTnjlENvr57RqL5-oxSqe_KGdbdL2MKhM'\n",
    "    sheet_name = 'Targets'\n",
    "    url = f'https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}'\n",
    "    targets = pd.read_csv(url)\n",
    "    targets['Date'] = pd.to_datetime(targets['Date']).dt.date\n",
    "\n",
    "    # Change Bonus Rates to 0, if Target Sales is not met\n",
    "    bonus = pd.merge(bonus, targets[['Store ID', 'Date', 'Target Sales', 'Bonus Rate']], on=['Store ID', 'Date'], how = 'left')\n",
    "    bonus['Bonus Rate'] = bonus['Bonus Rate'].where(bonus['Sales'] >= bonus['Target Sales'], 0)\n",
    "    bonus['Bonus'] = bonus['Bonus Rate']  * bonus['Hours']\n",
    "\n",
    "  # Work out additional_hr\n",
    "  additional_hr = additional_hr.dropna(subset=['Employee ID'])\n",
    "  additional_hr = pd.merge(additional_hr, employees[['Employee ID', 'First Name', 'Last Name', 'Company']], how='left', on=['Employee ID'])\n",
    "  additional_hr_col = [\n",
    "  'Employee ID',\n",
    "  'First Name',\n",
    "  'Last Name',\n",
    "  'Preferred Name',\n",
    "  'Company',\n",
    "  'Store',\n",
    "  'Date',\n",
    "  'Hours'\n",
    "  ]\n",
    "  additional_hr = additional_hr[additional_hr_col]\n",
    "  additional_hr['Date'] = pd.to_datetime(additional_hr['Date'])\n",
    "\n",
    "  # Concat rostered_hr and additional_hr\n",
    "  analysis = pd.concat([rostered_hr, additional_hr])\n",
    "\n",
    "  # Concat Bouns on to Timesheets\n",
    "  if bonus.shape[0]==0:\n",
    "     print('No Bonus Summary to calculate')\n",
    "     timesheets['Bonus']=0\n",
    "  else:\n",
    "    bonus_summary = bonus.groupby('Employee ID', as_index = False).agg({'Bonus':'sum'})\n",
    "    timesheets = pd.merge(timesheets, bonus_summary, on=['Employee ID'], how = 'left')\n",
    "    timesheets.fillna({'Bonus':0}, inplace = True)\n",
    "\n",
    "  upsheets = pd.melt(\n",
    "    timesheets, \n",
    "    id_vars=['Employee ID', 'First Name', 'Last Name', 'Company'],  # Columns to keep\n",
    "    value_vars=['Ord', 'Sat', 'Sun', 'Pub', 'Eve 1', 'Eve 2', 'No. of Shifts', 'Personal Leave', 'Annual Leave', 'Unpaid Leave', 'Bonus'],  # Columns to unpivot\n",
    "    var_name='type',\n",
    "    value_name='hours'\n",
    "  )\n",
    "\n",
    "  type_replacement = {\n",
    "    'Ord': 'Ordinary Hours',\n",
    "    'Sat': 'Saturday',\n",
    "    'Sun': 'Sunday',\n",
    "    'Pub': 'Public Holiday Hours',\n",
    "    'Eve 1': 'Late Evening Hours 10pm to Midnight',\n",
    "    'Eve 2': 'Late Evening Hours Midnight - 6AM',\n",
    "    'No. of Shifts': 'Laundry Allowance',\n",
    "    'Personal Leave': \"Personal/Carer's Leave\",\n",
    "    'Annual Leave': 'Annual Leave',\n",
    "    'Unpaid Leave': 'Other Unpaid Leave',\n",
    "    'Bonus': 'Bonus',\n",
    "  }\n",
    "  upsheets['type'] = upsheets['type'].replace(type_replacement)\n",
    "  \n",
    "  upsheets = upsheets[upsheets['hours'] != 0]\n",
    "  upsheets['date']=roster_start_date\n",
    "  upsheets = upsheets.rename(columns={'First Name': 'first_name', 'Last Name':'last_name'})\n",
    "\n",
    "\n",
    "  upsheets['rate'] = ''\n",
    "  upsheets['calculation_type'] = ''\n",
    "\n",
    "  fixed_hours_calc_types = [\"Personal/Carer's Leave\", 'Annual Leave', 'Other Unpaid Leave']\n",
    "  upsheets.loc[upsheets['type'].isin(fixed_hours_calc_types), 'calculation_type'] = 'FIXEDHOURS'\n",
    "\n",
    "  fixed_amount_calc_types = ['Bonus']\n",
    "  upsheets.loc[upsheets['type'].isin(fixed_amount_calc_types), 'calculation_type'] = 'FIXEDAMOUNT'\n",
    "  upsheets.loc[upsheets['type'].isin(fixed_amount_calc_types), 'rate'] = upsheets['hours']\n",
    "  upsheets.loc[upsheets['type'].isin(fixed_amount_calc_types), 'hours'] = ''\n",
    "\n",
    "  upsheets['digit'] = upsheets['Employee ID'].apply(count_digits_before_decimal)\n",
    "# Define a mapping of existing types to new types\n",
    "  casual_mapping = {\n",
    "      'Ordinary Hours': 'Casual Ordinary Hours',\n",
    "      'Saturday': 'Casual Saturday',\n",
    "      'Sunday': 'Casual Sunday',\n",
    "      'Public Holiday Hours': 'Casual Public Holiday Hours',\n",
    "      'Late Evening Hours 10pm to Midnight' : 'Casual Late Evening Hours 10pm to Midnight',\n",
    "      'Late Evening Hours Midnight - 6AM' : 'Casual Late Evening Hours Midnight - 6AM',\n",
    "  }\n",
    "# Use map with fillna to keep original values if no match in casual_mapping\n",
    "  upsheets.loc[upsheets['digit'] == 6, 'type'] = (\n",
    "      upsheets.loc[upsheets['digit'] == 6, 'type']\n",
    "      .map(casual_mapping)\n",
    "      .fillna(upsheets.loc[upsheets['digit'] == 6, 'type'])\n",
    "  )\n",
    "  GCM = upsheets[upsheets['Company']=='GCM']\n",
    "  HL = upsheets[upsheets['Company']=='HL']\n",
    "  SS = upsheets[upsheets['Company']=='SS']\n",
    "  MSC = upsheets[upsheets['Company']=='MSC']\n",
    "  WLD = upsheets[upsheets['Company']=='WLD']\n",
    "  upsheets_cols = ['Company','first_name', 'last_name', 'type', 'date','hours', 'rate', 'calculation_type']\n",
    "  upsheets = upsheets[upsheets_cols]\n",
    "\n",
    "  company_cols = ['first_name', 'last_name', 'type', 'date','hours', 'rate', 'calculation_type']\n",
    "  GCM = GCM[company_cols]\n",
    "  HL = HL[company_cols]\n",
    "  SS = SS[company_cols]\n",
    "  MSC = MSC[company_cols]\n",
    "  WLD = WLD[company_cols]\n",
    "\n",
    "  return timesheets, billings, over_threshold, analysis, bonus, upsheets, GCM, HL, SS, MSC, WLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # # END OF FUNCTIONS\n",
    "\n",
    "import io\n",
    "import streamlit as st\n",
    "\n",
    "st.title('Timesheet & Billing')\n",
    "\n",
    "uploaded_files = st.file_uploader(\"Choose Files\", accept_multiple_files = True)\n",
    "\n",
    "# Create an empty container\n",
    "output = st.empty()\n",
    "\n",
    "# if uploaded_files is not None:\n",
    "if len(uploaded_files) > 0:\n",
    "    ts, bl, ot, an, bo, up, gcm, hl, ss, msc, wld = calc_timesheets_n_billings(uploaded_files)\n",
    "\n",
    "    buffer = io.BytesIO()\n",
    "\n",
    "    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:\n",
    "    # Write each dataframe to a different worksheet.\n",
    "        ts.to_excel(writer, sheet_name='Timesheet', index = False)\n",
    "        bl.to_excel(writer, sheet_name='Billing', index = False)\n",
    "        ot.to_excel(writer, sheet_name='Over Threshold', index = False)\n",
    "        an.to_excel(writer, sheet_name='Analysis',index = False)\n",
    "        bo.to_excel(writer, sheet_name='Bonus',index = False)\n",
    "        up.to_excel(writer, sheet_name='Upsheets',index = False)\n",
    "        gcm.to_excel(writer, sheet_name='Upsheets GCM',index = False)\n",
    "        hl.to_excel(writer, sheet_name='Upsheets HL',index = False)\n",
    "        ss.to_excel(writer, sheet_name='Upsheets SS',index = False)\n",
    "        msc.to_excel(writer, sheet_name='Upsheets MSC',index = False)\n",
    "        wld.to_excel(writer, sheet_name='Upsheets WLD',index = False)\n",
    "\n",
    "    # Close the Pandas Excel writer and output the Excel file to the buffer\n",
    "    writer.close()\n",
    "\n",
    "    st.download_button(\n",
    "        label=\"Download\",\n",
    "        data=buffer,\n",
    "        file_name=\"Timesheet & Billing.xlsx\",\n",
    "        mime=\"application/vnd.ms-excel\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/eddy/Developer/Python/roster-consolidation/files/mscwq.xlsx']\n",
      "calc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddy/.pyenv/versions/3.13.0/envs/default/lib/python3.13/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "/Users/eddy/.pyenv/versions/3.13.0/envs/default/lib/python3.13/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "/Users/eddy/.pyenv/versions/3.13.0/envs/default/lib/python3.13/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "/Users/eddy/.pyenv/versions/3.13.0/envs/default/lib/python3.13/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bonus to process\n",
      "No Bonus Summary to calculate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3y/sylb4fpd4y19tc1y_nfdd_jw0000gn/T/ipykernel_58685/3006605791.py:282: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  upsheets.loc[upsheets['type'].isin(fixed_amount_calc_types), 'hours'] = ''\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory = '/Users/eddy/Developer/Python/roster-consolidation/files/'\n",
    "files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f)) and f.endswith('.xlsx')]\n",
    "print(files)\n",
    "ts, bl, ot, an, bo, up, gcm, hl, ss, msc, wld = calc_timesheets_n_billings(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>type</th>\n",
       "      <th>date</th>\n",
       "      <th>hours</th>\n",
       "      <th>rate</th>\n",
       "      <th>calculation_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Islay</td>\n",
       "      <td>Dunlop</td>\n",
       "      <td>Casual Ordinary Hours</td>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>7.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Islay</td>\n",
       "      <td>Dunlop</td>\n",
       "      <td>Laundry Allowance</td>\n",
       "      <td>2025-04-28</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_name last_name                   type       date hours rate  \\\n",
       "0      Islay    Dunlop  Casual Ordinary Hours 2025-04-28   7.5        \n",
       "6      Islay    Dunlop      Laundry Allowance 2025-04-28   1.0        \n",
       "\n",
       "  calculation_type  \n",
       "0                   \n",
       "6                   "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msc.tail(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
