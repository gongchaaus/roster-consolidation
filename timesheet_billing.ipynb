{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3y/sylb4fpd4y19tc1y_nfdd_jw0000gn/T/ipykernel_24233/3170424087.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import mysql.connector\n",
    "\n",
    "mysql_host = '34.116.84.145'\n",
    "mysql_port = '3306'\n",
    "mysql_user = 'gong-cha'\n",
    "mysql_password = 'HelloGongCha2012'\n",
    "mysql_database = 'gong_cha_redcat_db'\n",
    "\n",
    "# Engine for MySQL\n",
    "mysql_connection_string = f\"mysql+mysqlconnector://{mysql_user}:{mysql_password}@{mysql_host}:{mysql_port}/{mysql_database}\"\n",
    "mysql_engine = create_engine(mysql_connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # START OF FUNCTIONS\n",
    "\n",
    "def extract_additional_hr(file, sheet_name):\n",
    "  df = pd.read_excel(file, sheet_name = sheet_name)\n",
    "  df = df.dropna(subset=['Employee ID'])\n",
    "\n",
    "  if df['Employee ID'].dtypes == 'object':\n",
    "    df['Employee ID'] = df['Employee ID'].str[:6]\n",
    "  df['Employee ID'] = df['Employee ID'].astype(int)\n",
    "\n",
    "  if df['Store'].dtypes == float:\n",
    "    df['Store'] = df['Store'].astype(int).astype(str)\n",
    "\n",
    "  df = df.fillna({'Add': 0, 'Add.1': 0,'Add.2': 0,'Add.3': 0,'Add.4': 0,'Add.5': 0,'Personal Leave': 0,'Annual Leave': 0,})\n",
    "  df['2012-11-01'] = df['Add'] + df['Add.1'] + df['Add.2'] + df['Add.3'] + df['Add.4'] + df['Add.5']\n",
    "  df['2013-01-01'] = df['Personal Leave'] + df['Annual Leave']\n",
    "  cols =[0,1,2, df.columns.tolist().index('2012-11-01'),df.columns.tolist().index('2013-01-01')]\n",
    "\n",
    "  df = df.iloc[:, cols]\n",
    "  df = df.dropna(subset=df.columns[:3], how='all')\n",
    "  df = df.melt(id_vars=['Employee ID', 'Store', 'Preferred Name'], value_vars=df.columns[3:], var_name='Date', value_name='Hours')\n",
    "  df = df[df['Hours'] != 0]\n",
    "  return df\n",
    "\n",
    "def extract_rostered_hr(file, sheet_name):\n",
    "  df = pd.read_excel(file, sheet_name = sheet_name)\n",
    "\n",
    "  df = df.dropna(subset=['Employee ID'])\n",
    "\n",
    "  if df['Employee ID'].dtypes == 'object':\n",
    "    df['Employee ID'] = df['Employee ID'].str[:6]\n",
    "  df['Employee ID'] = df['Employee ID'].astype(int)\n",
    "\n",
    "  if df['Store'].dtypes == float:\n",
    "    df['Store'] = df['Store'].astype(int).astype(str)\n",
    "\n",
    "  df = df.iloc[:, :24]\n",
    "  cols = [0,1,2]\n",
    "  for col in range(3, 24, 3):\n",
    "      df[df.columns[col]] = df[df.columns[col+2]]\n",
    "      cols.append(col)\n",
    "  df = df.iloc[:,cols]\n",
    "  df = df.dropna(subset=df.columns[:3], how='all')\n",
    "  df = df.melt(id_vars=['Employee ID', 'Store', 'Preferred Name'], value_vars=df.columns[3:], var_name='Date', value_name='Hours')\n",
    "  df = df[df['Hours'] != 0]\n",
    "  return df\n",
    "\n",
    "# Function to count digits before the decimal point in a float number\n",
    "def count_digits_before_decimal(number):\n",
    "  # Convert to string, split at the decimal point, and count digits in the integer part\n",
    "  integer_part = str(number).split('.')[0]\n",
    "  return len(integer_part.replace('-', '').replace('nan', ''))\n",
    "\n",
    "def calc_timesheets_n_billings(files):\n",
    "  print('calc')\n",
    "  timesheets = pd.DataFrame()\n",
    "  billings = pd.DataFrame()\n",
    "  rostered_hr = pd.DataFrame()\n",
    "  additional_hr = pd.DataFrame()\n",
    "\n",
    "  for file in files:\n",
    "    timesheet = pd.read_excel(file, sheet_name = 'Timesheet')\n",
    "    billing = pd.read_excel(file, sheet_name = 'Billing')\n",
    "    rostered_hr_w1 = extract_rostered_hr(file, 'Week 1 Roster')\n",
    "    rostered_hr_w2 = extract_rostered_hr(file, 'Week 2 Roster')\n",
    "    additional_hr_w1 = extract_additional_hr(file, 'Week 1 Roster')\n",
    "    additional_hr_w2 = extract_additional_hr(file, 'Week 2 Roster')\n",
    "    employees = pd.read_excel(file, sheet_name = 'Employees')\n",
    "\n",
    "    timesheets = pd.concat([timesheets, timesheet], ignore_index=True)\n",
    "    billings = pd.concat([billings, billing], ignore_index=True)\n",
    "    rostered_hr = pd.concat([rostered_hr, rostered_hr_w1], ignore_index=True)\n",
    "    rostered_hr = pd.concat([rostered_hr, rostered_hr_w2], ignore_index=True)\n",
    "    additional_hr = pd.concat([additional_hr, additional_hr_w1], ignore_index=True)\n",
    "    additional_hr = pd.concat([additional_hr, additional_hr_w2], ignore_index=True)\n",
    "\n",
    "  #Remove irrelevant rows\n",
    "  timesheets = timesheets.dropna(subset = ['Employee ID'])\n",
    "\n",
    "  #Keep the needed columns\n",
    "  timesheets_cols = [1,2,3,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "  timesheets = timesheets[timesheets.columns[timesheets_cols]]\n",
    "  timesheets['Update Wage'] = timesheets['Update Wage'].astype(bool)\n",
    "  #Column Aggregations\n",
    "  timesheets_agg_cols = {'First Name':'first','Last Name':'first','Update Wage':'first','Hour Threshold':'first','Company':'first','Ord':'sum','Sat':'sum','Sun':'sum','Pub':'sum','Eve 1':'sum','Eve 2':'sum','No. of Shifts':'sum','Personal Leave':'sum','Annual Leave':'sum','Unpaid Leave':'sum','Total':'sum'}\n",
    "  timesheets = timesheets.groupby('Employee ID', as_index = False).agg(timesheets_agg_cols)\n",
    "  #Calculate Over Threshold\n",
    "  timesheets['Over Threshold'] = timesheets['Total'] - timesheets['Hour Threshold']\n",
    "  timesheets.loc[timesheets[\"Over Threshold\"] <=0, \"Over Threshold\"] = 0\n",
    "  #Keep Over Thresholds to a new df\n",
    "  over_threshold = timesheets[timesheets['Over Threshold']>0]\n",
    "  #Reduce Ord & Total with the excess\n",
    "  timesheets['Ord'] = timesheets['Ord'] - timesheets['Over Threshold']\n",
    "  timesheets['Total'] = timesheets['Total'] - timesheets['Over Threshold']\n",
    "  #Convert 80 & 100 hours to 76 hours\n",
    "  hours_col = ['Ord', 'Sat','Sun','Eve 1','Eve 2','Pub','Personal Leave', 'Annual Leave', 'Unpaid Leave', 'Total']\n",
    "  if(100 in timesheets[\"Hour Threshold\"].values):\n",
    "      timesheets.loc[timesheets[\"Hour Threshold\"] == 100, hours_col] = timesheets[hours_col]/100*76\n",
    "  if(80 in timesheets[\"Hour Threshold\"].values):\n",
    "      timesheets.loc[timesheets[\"Hour Threshold\"] == 80, hours_col] = timesheets[hours_col]/80*76\n",
    "  if any(timesheets[\"Hour Threshold\"] > 1000):\n",
    "  # Find rows where \"Hour Threshold\" is greater than 1000\n",
    "      rows_to_update = timesheets.loc[timesheets[\"Hour Threshold\"] > 1000]\n",
    "      # Perform actions on the rows\n",
    "      for index, row in rows_to_update.iterrows():\n",
    "          threshold = int(row[\"Hour Threshold\"])\n",
    "          base = int(str(threshold)[:2])\n",
    "          conversion = int(str(threshold)[-2:])\n",
    "          # Update multiple columns using .loc\n",
    "          timesheets.loc[index, hours_col] = timesheets.loc[index, hours_col] / conversion * base\n",
    "\n",
    "  #drop Hour Threshold & Over Threshold\n",
    "  timesheets = timesheets.drop(['Hour Threshold','Over Threshold'],axis = 1)\n",
    "\n",
    "  #Remove irrelevant rows\n",
    "  billings.dropna(subset=['Store'],inplace = True)\n",
    "  billings = billings[billings['Total'] > 0]\n",
    "\n",
    "  #Keep the needed columns\n",
    "  billings_cols = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "  billings = billings[billings.columns[billings_cols]]\n",
    "  #Column Aggregations\n",
    "  billings_agg_cols = {'Ord':'sum','Sat':'sum','Sun':'sum','Pub':'sum','Eve 1':'sum','Eve 2':'sum','No. of Shifts':'sum','Personal Leave':'sum','Annual Leave':'sum','Unpaid Leave':'sum','Total':'sum'}\n",
    "  billings = billings.groupby('Store', as_index = False).agg(billings_agg_cols)\n",
    "\n",
    "  rostered_hr = pd.merge(rostered_hr, employees[['Employee ID', 'First Name', 'Last Name', 'Company']], how='left', on=['Employee ID'])\n",
    "  rostered_hr_col = [\n",
    "  'Employee ID',\n",
    "  'First Name',\n",
    "  'Last Name',\n",
    "  'Preferred Name',\n",
    "  'Company',\n",
    "  'Store',\n",
    "  'Date',\n",
    "  'Hours'\n",
    "  ]\n",
    "  rostered_hr = rostered_hr[rostered_hr_col]\n",
    "  rostered_hr['Date'] = pd.to_datetime(rostered_hr['Date']).dt.date\n",
    "\n",
    "  bonus = rostered_hr.copy()\n",
    "\n",
    "  # Stitch Store ID and drop rows which Store ID are not found\n",
    "  sheet_id = '1rqOeBjA9drmTnjlENvr57RqL5-oxSqe_KGdbdL2MKhM'\n",
    "  sheet_name = 'StoreReference'\n",
    "  url = f'https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}'\n",
    "  store_ref = pd.read_csv(url)\n",
    "\n",
    "  bonus = pd.merge(bonus, store_ref, on=['Store'], how = 'left')\n",
    "\n",
    "  bonus.dropna(subset=['Store ID'], inplace = True)\n",
    "\n",
    "  # Stitch recid_plo\n",
    "  sheet_id = '1aVcnah9Cp_PUvFiXgd2XRmBpLhumCqHUaqROaLcpYfc'\n",
    "  sheet_name = 'store_id_ref'\n",
    "  url = f'https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}'\n",
    "  store_id_ref_crm = pd.read_csv(url)\n",
    "  store_id_ref_crm = store_id_ref_crm.rename(columns={'store_id' : 'Store ID'})\n",
    "  bonus = pd.merge(bonus, store_id_ref_crm[['Store ID', 'recid_plo']], on=['Store ID'], how = 'left')\n",
    "  bonus['recid_plo'] = bonus['recid_plo'].astype(int)\n",
    "\n",
    "  # Stich sales based on recid_plo & dates, skip if there is no Date\n",
    "  start = bonus['Date'].min()\n",
    "  end = bonus['Date'].max()\n",
    "\n",
    "  start_str = start.strftime('%Y-%m-%d')\n",
    "  end_str = end.strftime('%Y-%m-%d')\n",
    "\n",
    "  recid_plo_list = bonus['recid_plo'].unique().tolist()\n",
    "  recid_plo_list_str = ', '.join(str(id) for id in recid_plo_list)\n",
    "\n",
    "  sheet_id = '1peA8effpeSTk3duIjxF46V-PrDD8tv3fubTCDEpD940'\n",
    "  sheet_name = 'ops_bonus_exclusion'\n",
    "  url = f'https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}'\n",
    "  exclusion_df = pd.read_csv(url)\n",
    "  excluded_recid_plu = exclusion_df['recid_plu'].drop_duplicates()\n",
    "\n",
    "  excluded_recid_plu_str = ', '.join(str(s) for s in excluded_recid_plu)\n",
    "\n",
    "  query = '''\n",
    "  SELECT ts2.recid_plo, ts.itemdate as Date, sum(ts.qty*ts.price) as Sales\n",
    "  FROM tbl_salesitems ts \n",
    "  JOIN tbl_salesheaders ts2 on ts.recid_mixh = ts2.recid\n",
    "  WHERE ts.itemdate >= '{start}' and ts.itemdate <= '{end}' and ts2.recid_plo in ({recid_plo_list}) and ts.recid_plu not in ({excluded_recid_plu})\n",
    "  GROUP BY ts2.recid_plo, ts.itemdate\n",
    "  ORDER BY ts.itemdate ASC, recid_plo ASC\n",
    "  '''.format(start=start_str, end = end_str, recid_plo_list = recid_plo_list_str, excluded_recid_plu = excluded_recid_plu_str)\n",
    "\n",
    "  sales_df = pd.read_sql(query, mysql_engine)\n",
    "  sales_df['Date'] = pd.to_datetime(sales_df['Date']).dt.date\n",
    "\n",
    "  bonus = pd.merge(bonus, sales_df[['recid_plo', 'Date', 'Sales']], on=['recid_plo', 'Date'], how = 'left')\n",
    "\n",
    "  # Stitch Target Sales & Bonus Rates\n",
    "  sheet_id = '1rqOeBjA9drmTnjlENvr57RqL5-oxSqe_KGdbdL2MKhM'\n",
    "  sheet_name = 'Targets'\n",
    "  url = f'https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}'\n",
    "  targets = pd.read_csv(url)\n",
    "  targets['Date'] = pd.to_datetime(targets['Date']).dt.date\n",
    "\n",
    "  # Change Bonus Rates to 0, if Target Sales is not met\n",
    "  bonus = pd.merge(bonus, targets[['Store ID', 'Date', 'Target Sales', 'Bonus Rate']], on=['Store ID', 'Date'], how = 'left')\n",
    "  bonus['Bonus Rate'] = bonus['Bonus Rate'].where(bonus['Sales'] >= bonus['Target Sales'], 0)\n",
    "  bonus['Bonus'] = bonus['Bonus Rate']  * bonus['Hours']\n",
    "\n",
    "  # Work out additional_hr\n",
    "  additional_hr = additional_hr.dropna(subset=['Employee ID'])\n",
    "  additional_hr = pd.merge(additional_hr, employees[['Employee ID', 'First Name', 'Last Name', 'Company']], how='left', on=['Employee ID'])\n",
    "  additional_hr_col = [\n",
    "  'Employee ID',\n",
    "  'First Name',\n",
    "  'Last Name',\n",
    "  'Preferred Name',\n",
    "  'Company',\n",
    "  'Store',\n",
    "  'Date',\n",
    "  'Hours'\n",
    "  ]\n",
    "  additional_hr = additional_hr[additional_hr_col]\n",
    "  additional_hr['Date'] = pd.to_datetime(additional_hr['Date'])\n",
    "\n",
    "  # Concat rostered_hr and additional_hr\n",
    "  analysis = pd.concat([rostered_hr, additional_hr])\n",
    "\n",
    "  # Concat Bouns on to Timesheets\n",
    "  bonus_summary = bonus.groupby('Employee ID', as_index = False).agg({'Bonus':'sum'})\n",
    "  timesheets = pd.merge(timesheets, bonus_summary, on=['Employee ID'], how = 'left')\n",
    "  timesheets.fillna({'Bonus':0}, inplace = True)\n",
    "\n",
    "  upsheets = pd.melt(\n",
    "    timesheets, \n",
    "    id_vars=['Employee ID', 'First Name', 'Last Name', 'Company'],  # Columns to keep\n",
    "    value_vars=['Ord', 'Sat', 'Sun', 'Pub', 'Eve 1', 'Eve 2', 'No. of Shifts', 'Personal Leave', 'Annual Leave', 'Unpaid Leave', 'Bonus'],  # Columns to unpivot\n",
    "    var_name='type',\n",
    "    value_name='hours'\n",
    "  )\n",
    "\n",
    "  type_replacement = {\n",
    "    'Ord': 'Ordinary Hours',\n",
    "    'Sat': 'Saturday',\n",
    "    'Sun': 'Sunday',\n",
    "    'Pub': 'Public Holiday Hours',\n",
    "    'Eve 1': 'Late Evening Hours 10pm to Midnight',\n",
    "    'Eve 2': 'Late Evening Hours Midnight - 6AM',\n",
    "    'No. of Shifts': 'Laundry Allowance',\n",
    "    'Personal Leave': \"Personal/Carer's Leave\",\n",
    "    'Annual Leave': 'Annual Leave',\n",
    "    'Unpaid Leave': 'Other Unpaid Leave',\n",
    "    'Bonus': 'Bonus',\n",
    "  }\n",
    "  upsheets['type'] = upsheets['type'].replace(type_replacement)\n",
    "  \n",
    "  upsheets = upsheets[upsheets['hours'] != 0]\n",
    "  upsheets['date']=bonus['Date'].min()\n",
    "  upsheets = upsheets.rename(columns={'First Name': 'first_name', 'Last Name':'last_name'})\n",
    "\n",
    "\n",
    "  upsheets['rate'] = ''\n",
    "  upsheets['calculation_type'] = ''\n",
    "\n",
    "  fixed_hours_calc_types = [\"Personal/Carer's Leave\", 'Annual Leave', 'Other Unpaid Leave']\n",
    "  upsheets.loc[upsheets['type'].isin(fixed_hours_calc_types), 'calculation_type'] = 'FIXEDHOURS'\n",
    "\n",
    "  fixed_amount_calc_types = ['Bonus']\n",
    "  upsheets.loc[upsheets['type'].isin(fixed_amount_calc_types), 'calculation_type'] = 'FIXEDAMOUNT'\n",
    "  upsheets.loc[upsheets['type'].isin(fixed_amount_calc_types), 'rate'] = upsheets['hours']\n",
    "  upsheets.loc[upsheets['type'].isin(fixed_amount_calc_types), 'hours'] = ''\n",
    "\n",
    "  upsheets['digit'] = upsheets['Employee ID'].apply(count_digits_before_decimal)\n",
    "# Define a mapping of existing types to new types\n",
    "  casual_mapping = {\n",
    "      'Ordinary Hours': 'Casual Ordinary Hours',\n",
    "      'Saturday': 'Casual Saturday',\n",
    "      'Sunday': 'Casual Sunday',\n",
    "      'Public Holiday Hours': 'Casual Public Holiday Hours',\n",
    "      'Late Evening Hours 10pm to Midnight' : 'Casual Late Evening Hours 10pm to Midnight',\n",
    "      'Late Evening Hours Midnight - 6AM' : 'Casual Late Evening Hours Midnight - 6AM',\n",
    "  }\n",
    "# Use map with fillna to keep original values if no match in casual_mapping\n",
    "  upsheets.loc[upsheets['digit'] == 6, 'type'] = (\n",
    "      upsheets.loc[upsheets['digit'] == 6, 'type']\n",
    "      .map(casual_mapping)\n",
    "      .fillna(upsheets.loc[upsheets['digit'] == 6, 'type'])\n",
    "  )\n",
    "  GCM = upsheets[upsheets['Company']=='GCM']\n",
    "  HL = upsheets[upsheets['Company']=='HL']\n",
    "  SS = upsheets[upsheets['Company']=='SS']\n",
    "  upsheets_cols = ['Company','first_name', 'last_name', 'type', 'date','hours', 'rate', 'calculation_type']\n",
    "  upsheets = upsheets[upsheets_cols]\n",
    "\n",
    "  company_cols = ['first_name', 'last_name', 'type', 'date','hours', 'rate', 'calculation_type']\n",
    "  GCM = GCM[company_cols]\n",
    "  HL = HL[company_cols]\n",
    "  SS = SS[company_cols]\n",
    "\n",
    "  return timesheets, billings, over_threshold, analysis, bonus, upsheets, GCM, HL, SS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # # END OF FUNCTIONS\n",
    "\n",
    "\n",
    "import io\n",
    "import streamlit as st\n",
    "\n",
    "st.title('Timesheet & Billing')\n",
    "\n",
    "uploaded_files = st.file_uploader(\"Choose Files\", accept_multiple_files = True)\n",
    "\n",
    "# Create an empty container\n",
    "output = st.empty()\n",
    "\n",
    "# if uploaded_files is not None:\n",
    "if len(uploaded_files) > 0:\n",
    "    ts, bl, ot, an, bo, up, gcm, hl, ss = calc_timesheets_n_billings(uploaded_files)\n",
    "\n",
    "    buffer = io.BytesIO()\n",
    "\n",
    "    with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:\n",
    "    # Write each dataframe to a different worksheet.\n",
    "        ts.to_excel(writer, sheet_name='Timesheet', index = False)\n",
    "        bl.to_excel(writer, sheet_name='Billing', index = False)\n",
    "        ot.to_excel(writer, sheet_name='Over Threshold', index = False)\n",
    "        an.to_excel(writer, sheet_name='Analysis',index = False)\n",
    "        bo.to_excel(writer, sheet_name='Bonus',index = False)\n",
    "        up.to_excel(writer, sheet_name='Upsheets',index = False)\n",
    "        gcm.to_excel(writer, sheet_name='Upsheets GCM',index = False)\n",
    "        hl.to_excel(writer, sheet_name='Upsheets HL',index = False)\n",
    "        ss.to_excel(writer, sheet_name='Upsheets SS',index = False)\n",
    "\n",
    "    # Close the Pandas Excel writer and output the Excel file to the buffer\n",
    "    writer.close()\n",
    "\n",
    "    st.download_button(\n",
    "        label=\"Download\",\n",
    "        data=buffer,\n",
    "        file_name=\"Timesheet & Billing.xlsx\",\n",
    "        mime=\"application/vnd.ms-excel\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eddy/.pyenv/versions/3.12.4/envs/default/lib/python3.12/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "/Users/eddy/.pyenv/versions/3.12.4/envs/default/lib/python3.12/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "/Users/eddy/.pyenv/versions/3.12.4/envs/default/lib/python3.12/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "/Users/eddy/.pyenv/versions/3.12.4/envs/default/lib/python3.12/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "/var/folders/3y/sylb4fpd4y19tc1y_nfdd_jw0000gn/T/ipykernel_94112/2633038883.py:266: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  upsheets.loc[upsheets['type'].isin(fixed_amount_calc_types), 'hours'] = ''\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory = '/Users/eddy/Developer/Python/roster-consolidation/files/'\n",
    "files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f)) and f.endswith('.xlsx')]\n",
    "\n",
    "ts, bl, ot, an, bo, up, gcm, hl, ss = calc_timesheets_n_billings(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>type</th>\n",
       "      <th>date</th>\n",
       "      <th>hours</th>\n",
       "      <th>rate</th>\n",
       "      <th>calculation_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HL</td>\n",
       "      <td>Adrian</td>\n",
       "      <td>Lim</td>\n",
       "      <td>Ordinary Hours</td>\n",
       "      <td>2024-08-19</td>\n",
       "      <td>41.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HL</td>\n",
       "      <td>Alma</td>\n",
       "      <td>Espinoza</td>\n",
       "      <td>Ordinary Hours</td>\n",
       "      <td>2024-08-19</td>\n",
       "      <td>24.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HL</td>\n",
       "      <td>Criselle Joy</td>\n",
       "      <td>Sico</td>\n",
       "      <td>Ordinary Hours</td>\n",
       "      <td>2024-08-19</td>\n",
       "      <td>13.666667</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HL</td>\n",
       "      <td>Emmanuel</td>\n",
       "      <td>Quebec</td>\n",
       "      <td>Ordinary Hours</td>\n",
       "      <td>2024-08-19</td>\n",
       "      <td>8.133333</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HL</td>\n",
       "      <td>Georgiana</td>\n",
       "      <td>Nhan</td>\n",
       "      <td>Ordinary Hours</td>\n",
       "      <td>2024-08-19</td>\n",
       "      <td>17.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>HL</td>\n",
       "      <td>Jiuri</td>\n",
       "      <td>Palahang</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-08-19</td>\n",
       "      <td></td>\n",
       "      <td>11.65</td>\n",
       "      <td>FIXEDAMOUNT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>HL</td>\n",
       "      <td>Chelcy</td>\n",
       "      <td>Hargreaves</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-08-19</td>\n",
       "      <td></td>\n",
       "      <td>14.0</td>\n",
       "      <td>FIXEDAMOUNT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>HL</td>\n",
       "      <td>Elyssa</td>\n",
       "      <td>Acallar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-08-19</td>\n",
       "      <td></td>\n",
       "      <td>5.0</td>\n",
       "      <td>FIXEDAMOUNT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>HL</td>\n",
       "      <td>Mikayla</td>\n",
       "      <td>Perez</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-08-19</td>\n",
       "      <td></td>\n",
       "      <td>12.5</td>\n",
       "      <td>FIXEDAMOUNT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>HL</td>\n",
       "      <td>Heleana</td>\n",
       "      <td>Soriano</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-08-19</td>\n",
       "      <td></td>\n",
       "      <td>19.166667</td>\n",
       "      <td>FIXEDAMOUNT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Company    first_name   last_name            type        date      hours  \\\n",
       "0        HL        Adrian         Lim  Ordinary Hours  2024-08-19       41.5   \n",
       "1        HL          Alma    Espinoza  Ordinary Hours  2024-08-19       24.0   \n",
       "3        HL  Criselle Joy        Sico  Ordinary Hours  2024-08-19  13.666667   \n",
       "4        HL      Emmanuel      Quebec  Ordinary Hours  2024-08-19   8.133333   \n",
       "5        HL     Georgiana        Nhan  Ordinary Hours  2024-08-19       17.0   \n",
       "..      ...           ...         ...             ...         ...        ...   \n",
       "325      HL         Jiuri    Palahang             NaN  2024-08-19              \n",
       "326      HL        Chelcy  Hargreaves             NaN  2024-08-19              \n",
       "327      HL        Elyssa     Acallar             NaN  2024-08-19              \n",
       "328      HL       Mikayla       Perez             NaN  2024-08-19              \n",
       "329      HL       Heleana     Soriano             NaN  2024-08-19              \n",
       "\n",
       "          rate calculation_type  \n",
       "0                                \n",
       "1                                \n",
       "3                                \n",
       "4                                \n",
       "5                                \n",
       "..         ...              ...  \n",
       "325      11.65      FIXEDAMOUNT  \n",
       "326       14.0      FIXEDAMOUNT  \n",
       "327        5.0      FIXEDAMOUNT  \n",
       "328       12.5      FIXEDAMOUNT  \n",
       "329  19.166667      FIXEDAMOUNT  \n",
       "\n",
       "[114 rows x 8 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
